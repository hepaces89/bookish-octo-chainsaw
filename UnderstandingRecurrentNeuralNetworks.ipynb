{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Understanding Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks are an evolution of Artificial Neural Networks (ANNs) that are designed to be used on sequences of data where the prior data values are expected to have some affect on the next value in the sequence. [^1] In a sense they kind of remind me of Hidden Markov Models (HMMs)\n",
    "Please note that we should not confuse RNNs with Recursive Neural Networks (RvNNs) which are a completely different type of neural network. [^2]\n",
    "\n",
    "As shown in the diagram below, a simple RNN differs from an ANN in the sense that the output of a node from the prior element is combined with the input for the next element.\n",
    "This means that the RNN considers information for both the current input and the output from the prior input.\n",
    "\n",
    "![RNN vs ANN structure comparison](./recurrentVsFeedForwardNNs.png) [^4]\n",
    "\n",
    "RNN types:\n",
    " - 1 to 1\n",
    " - 1 to many\n",
    " - many to 1\n",
    " - many to many\n",
    "\n",
    "![RNN types](./rnnTypes.png) [^4]\n",
    "\n",
    "## Training\n",
    "\n",
    "Like with regular neural networks, RNNs are trained with a modified version of the general Forward Propagation \n",
    "and Back Propagation process called `Back Propagation Through Time` (BPTT)\n",
    "\n",
    "Forward propagation consists of putting a input into the ANN and then calculating the \"error\" between the calculated result and the expected result.\n",
    "Back Propagation occurs when we take that error and work our way back through the neural net updating the weights on each node to minimize the total error.\n",
    "\n",
    "\n",
    "## Bibliography/Resources\n",
    "\n",
    "[^1]: [Recurrent Neural Networks - Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "\n",
    "[^2]: [Recursive Neural Networks - Wikipedia](https://en.wikipedia.org/wiki/Recursive_neural_network)\n",
    "\n",
    "[^3]: [Illustrated Guide to RNNs - Towards Data Science](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
    "\n",
    "[^4]: [RNNs and LSTM - BuiltIn.com](https://builtin.com/data-science/recurrent-neural-networks-and-lstm)\n",
    "\n",
    "[^5]: [RNNs Cheatsheet - Stanford.com](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "\n",
    "[^6]: [RNNs with Keras - Tensorflow.com](https://www.tensorflow.org/guide/keras/rnn)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ANN from scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cProfile"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "mucking around with tensor flow basics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x = [[2]]\n",
    "m = tf.matmul(x,x)\n",
    "print(f'm = {m}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "m = [[4]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "mucking around with back prop: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "![back prop by hand pg 01](./backprop_by_hand/backprop_by_hand_01.jpg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# initial weights (input and bias) for hidden layer's node 1\n",
    "weights_h1 = [[0.15, 0.2,1]]\n",
    "# inputs and bias going into hidden layer's node 1\n",
    "inputs = [[0.05],[0.1],[0.35]]\n",
    "# the matrix multiplication to generate the (total) net input\n",
    "net_input_h1 = tf.matmul(weights_h1, inputs)\n",
    "print(f'net_input_h1: {net_input_h1}')\n",
    "\n",
    "# calculate the output by running the net input through the activation function (in this case the logistic/sigmoid func)\n",
    "out_h1 = 1 / (1 + tf.math.exp(-1*net_input_h1))\n",
    "print(f'out_h1{out_h1}')\n",
    "print(f'using the built in sigmoid func: {tf.math.sigmoid(net_input_h1)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "net_input_h1: [[0.3775]]\n",
      "out_h1[[0.59327]]\n",
      "using the built in sigmoid func: [[0.59327]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![back prop by hand pg 02](./backprop_by_hand/backprop_by_hand_02.jpg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# repeating the process for node 2 of the hidden layer\n",
    "weights_h2 = [[0.25, 0.3,1]]\n",
    "net_input_h2 = tf.matmul(weights_h2, inputs)\n",
    "print(f'net_input_h2: {net_input_h2}')\n",
    "out_h2 = tf.math.sigmoid(net_input_h2)\n",
    "print(f'out_h2: {out_h2}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "net_input_h2: [[0.39249998]]\n",
      "out_h2: [[0.59688437]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# repeating the process for node 1 of the output layer\n",
    "weights_o1 = [[0.4, 0.45, 1]]\n",
    "hidden_result = [out_h1[0],out_h2[0],[0.6]]\n",
    "net_input_o1 = tf.matmul(weights_o1, hidden_result)\n",
    "print(f'net_input_o1: {net_input_o1}')\n",
    "out_o1 = tf.math.sigmoid(net_input_o1)\n",
    "print(f'out_o1: {out_o1}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "net_input_o1: [[1.105906]]\n",
      "out_o1: [[0.75136507]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# repeating process for node 2 of the output layer\n",
    "weights_o2 = [[0.5, 0.55, 1]]\n",
    "net_input_o2 = tf.matmul(weights_o2, hidden_result)\n",
    "print(f'net_input_o2:{net_input_o2}')\n",
    "out_o2 = tf.math.sigmoid(net_input_o2)\n",
    "print(f'out_o2:{out_o2}')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "net_input_o2:[[1.2249215]]\n",
      "out_o2:[[0.7729285]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculating the Error\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Then we can calculate the errors\n",
    "\n",
    "target_o1 = 0.01\n",
    "error_o1 = 0.5 * tf.math.pow((target_o1 - out_o1),2)\n",
    "print(f'error_o1: {error_o1}')\n",
    "\n",
    "target_o2 = 0.99\n",
    "error_o2 = 0.5 * tf.math.pow((target_o2 - out_o2),2)\n",
    "print(f'error_o2: {error_o2}')\n",
    "\n",
    "error_total = error_o1 + error_o2\n",
    "print(f'error_total: {error_total}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error_o1: [[0.2748111]]\n",
      "error_o2: [[0.02356002]]\n",
      "error_total: [[0.2983711]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Executing the backwards pass of back propagation\n",
    "\n",
    "![back prop by hand pg 03](./backprop_by_hand/backprop_by_hand_03.jpg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![back prop by hand pg 04](./backprop_by_hand/backprop_by_hand_04.jpg)\n",
    "\n",
    "![back prop by hand pg 05](./backprop_by_hand/backprop_by_hand_05.jpg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "d_error_total_p_d_out_o1 = out_o1 - target_o1\n",
    "d_out_o1_p_d_net_o1 = out_o1 * (1 - out_o1)\n",
    "d_net_o1_p_d_w5 = out_h1\n",
    "d_error_total_p_d_w5 = d_error_total_p_d_out_o1 * d_out_o1_p_d_net_o1 * d_net_o1_p_d_w5\n",
    "print(f'd_error_total_p_d_w5: {d_error_total_p_d_w5}')\n",
    "d_net_o1_p_d_w6 = out_h2\n",
    "d_error_total_p_d_w6 = d_error_total_p_d_out_o1 * d_out_o1_p_d_net_o1 * d_net_o1_p_d_w6\n",
    "print(f'd_error_total_p_d_w6: {d_error_total_p_d_w6}')\n",
    "\n",
    "w6n = 0.45-0.5*d_error_total_p_d_w6\n",
    "print(f'w6n: {w6n}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d_error_total_p_d_w5: [[0.08216704]]\n",
      "d_error_total_p_d_w6: [[0.08266763]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('tensor-env': venv)"
  },
  "interpreter": {
   "hash": "fce8f47ca1d4ed8d1e3774617e0d70568bbab428daebe9507895c0618f64b636"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}