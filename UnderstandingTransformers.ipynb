{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Path to understanding Transformers\n",
    "\n",
    "Apparently its some sort of new neural network architecture, that at first glance looks like a really complicated Hidden Markov setup.\n",
    "Looks like it evolved from special type of Recursive Neural Network (RNN) called a Long Short Term Memory (LSTM Neural Network)\n",
    "Unlike the convolutional neural network (CNNs) that I am more familar with instead of training convolving filters to break down images into components\n",
    "we are processing sequences of inputs?\n",
    "\n",
    "Apparently it consists of two major components, an \"encoder\" stack and a \"decoder\" stack. \n",
    "\n",
    "## Quick Google Search Results:\n",
    " - [Towards DataScience Transformer Breakdown](https://towardsdatascience.com/transformer-neural-network-step-by-step-breakdown-of-the-beast-b3e096dc857f)\n",
    " - [Illustrated Transformer Lecture/Article focused on language model](https://jalammar.github.io/illustrated-transformer/)\n",
    "    - [Attention?](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "\n",
    "background: https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}